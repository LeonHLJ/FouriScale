# T_references for training size (512)
Tref = {
    "down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor": {4096},
    "down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor": {77},
    "down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor": {4096},
    "down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor": {77},
    "down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor": {1024},
    "down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor": {77},
    "down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor": {1024},
    "down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor": {77},
    "down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor": {256},
    "down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor": {77},
    "down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor": {256},
    "down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor": {77},
    "mid_block.attentions.0.transformer_blocks.0.attn1.processor": {64},
    "mid_block.attentions.0.transformer_blocks.0.attn2.processor": {77},
    "up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor": {256},
    "up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor": {77},
    "up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor": {256},
    "up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor": {77},
    "up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor": {256},
    "up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor": {77},
    "up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor": {1024},
    "up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor": {77},
    "up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor": {1024},
    "up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor": {77},
    "up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor": {1024},
    "up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor": {77},
    "up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor": {4096},
    "up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor": {77},
    "up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor": {4096},
    "up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor": {77},
    "up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor": {4096},
    "up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor": {77},
}

list_layers = [
    "down_blocks.0.attentions.0.transformer_blocks.0.attn1",
    "down_blocks.0.attentions.0.transformer_blocks.0.attn2",
    "down_blocks.0.attentions.1.transformer_blocks.0.attn1",
    "down_blocks.0.attentions.1.transformer_blocks.0.attn2",
    "down_blocks.1.attentions.0.transformer_blocks.0.attn1",
    "down_blocks.1.attentions.0.transformer_blocks.0.attn2",
    "down_blocks.1.attentions.1.transformer_blocks.0.attn1",
    "down_blocks.1.attentions.1.transformer_blocks.0.attn2",
    "down_blocks.2.attentions.0.transformer_blocks.0.attn1",
    "down_blocks.2.attentions.0.transformer_blocks.0.attn2",
    "down_blocks.2.attentions.1.transformer_blocks.0.attn1",
    "down_blocks.2.attentions.1.transformer_blocks.0.attn2",
    "up_blocks.1.attentions.0.transformer_blocks.0.attn1",
    "up_blocks.1.attentions.0.transformer_blocks.0.attn2",
    "up_blocks.1.attentions.1.transformer_blocks.0.attn1",
    "up_blocks.1.attentions.1.transformer_blocks.0.attn2",
    "up_blocks.1.attentions.2.transformer_blocks.0.attn1",
    "up_blocks.1.attentions.2.transformer_blocks.0.attn2",
    "up_blocks.2.attentions.0.transformer_blocks.0.attn1",
    "up_blocks.2.attentions.0.transformer_blocks.0.attn2",
    "up_blocks.2.attentions.1.transformer_blocks.0.attn1",
    "up_blocks.2.attentions.1.transformer_blocks.0.attn2",
    "up_blocks.2.attentions.2.transformer_blocks.0.attn1",
    "up_blocks.2.attentions.2.transformer_blocks.0.attn2",
    "up_blocks.3.attentions.0.transformer_blocks.0.attn1",
    "up_blocks.3.attentions.0.transformer_blocks.0.attn2",
    "up_blocks.3.attentions.1.transformer_blocks.0.attn1",
    "up_blocks.3.attentions.1.transformer_blocks.0.attn2",
    "up_blocks.3.attentions.2.transformer_blocks.0.attn1",
    "up_blocks.3.attentions.2.transformer_blocks.0.attn2",
    "mid_block.attentions.0.transformer_blocks.0.attn1",
    "mid_block.attentions.0.transformer_blocks.0.attn2",
]
list_layers = list(map(lambda x: x + ".processor", list_layers))
